{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EIP3_Assignment_6.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unography/EIP3-Assignments/blob/master/EIP3_Assignment_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtNbphuTvjbq",
        "colab_type": "code",
        "outputId": "c7cd4bed-0802-43d7-c494-a4106e3f5dfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/Drive')\n",
        "\n",
        "!cp /Drive/My\\ Drive/alice.txt ."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /Drive; to attempt to forcibly remount, call drive.mount(\"/Drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSCppbLGBMZY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Od7qTxNfvm2Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = \"alice.txt\"\n",
        "raw_text = open(filename).read()\n",
        "raw_text = raw_text.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "425_pWtoCOof",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_punc(text):\n",
        "  return re.sub('[^A-Za-z0-9. \\n]+', '', text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvaIUHy7CVHz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_text = remove_punc(raw_text)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE7CPu4Zv1Op",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiwLW3v4wB74",
        "colab_type": "code",
        "outputId": "d318fcbc-ca4b-47fc-c6a4-05e5747eaea6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  137077\n",
            "Total Vocab:  31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1XmvDbLFD2w",
        "colab_type": "code",
        "outputId": "522514b6-b0d9-4867-9c91-e63f61bd0ed9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sentence_split = [x.split('.') for x in raw_text.split('\\n')]\n",
        "print(len(sentence_split))\n",
        "sentences = []\n",
        "for sent in sentence_split:\n",
        "  for each_sent in sent:\n",
        "    if each_sent:\n",
        "      each_sent = each_sent.strip()\n",
        "      each_sent += \".\"\n",
        "      sentences.append(each_sent)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3337\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAg6oIVDFHD9",
        "colab_type": "code",
        "outputId": "410c923e-5555-4b13-9b7a-05070d0ca50f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sentences[0], len(sentences)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('alices adventures in wonderland.', 2849)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oo1hSqZeNYkm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataX = []\n",
        "dataY = []\n",
        "seq_length = 30\n",
        "for sent in sentences:\n",
        "  if len(sent) <= seq_length:\n",
        "    dataX.append([char_to_int[char] for char in sent[:-1]])\n",
        "    dataY.append(char_to_int[sent[-1]])\n",
        "  else:\n",
        "    for i in range(0, len(sent) - seq_length, 1):\n",
        "      seq_in = sent[i:i + seq_length]\n",
        "      seq_out = sent[i + seq_length]\n",
        "      dataX.append([char_to_int[char] for char in seq_in])\n",
        "      dataY.append(char_to_int[seq_out])\n",
        "\n",
        "seq_length = 30\n",
        "dataX = pad_sequences(dataX, maxlen=seq_length)\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3luREaWDwuQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rH4Am9ojEI-A",
        "colab_type": "code",
        "outputId": "fee28409-ef9e-43e9-b7e0-62edb668d045",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(59825, 30, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BxvONAeEKQy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFQC0J3QVC1w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint = ModelCheckpoint('model.h5', monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgFVT_w6VSsC",
        "colab_type": "code",
        "outputId": "37d7307e-99b0-45ba-fc74-25b42d6ba57a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X, y, epochs=100, batch_size=128, callbacks=callbacks_list)"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "59825/59825 [==============================] - 44s 728us/step - loss: 2.8881\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.88806, saving model to model.h5\n",
            "Epoch 2/100\n",
            "59825/59825 [==============================] - 38s 643us/step - loss: 2.7332\n",
            "\n",
            "Epoch 00002: loss improved from 2.88806 to 2.73320, saving model to model.h5\n",
            "Epoch 3/100\n",
            "59825/59825 [==============================] - 38s 643us/step - loss: 2.6383\n",
            "\n",
            "Epoch 00003: loss improved from 2.73320 to 2.63833, saving model to model.h5\n",
            "Epoch 4/100\n",
            "59825/59825 [==============================] - 39s 649us/step - loss: 2.5416\n",
            "\n",
            "Epoch 00004: loss improved from 2.63833 to 2.54162, saving model to model.h5\n",
            "Epoch 5/100\n",
            "59825/59825 [==============================] - 40s 673us/step - loss: 2.4666\n",
            "\n",
            "Epoch 00005: loss improved from 2.54162 to 2.46665, saving model to model.h5\n",
            "Epoch 6/100\n",
            "59825/59825 [==============================] - 39s 658us/step - loss: 2.3829\n",
            "\n",
            "Epoch 00006: loss improved from 2.46665 to 2.38287, saving model to model.h5\n",
            "Epoch 7/100\n",
            "59825/59825 [==============================] - 39s 648us/step - loss: 2.3180\n",
            "\n",
            "Epoch 00007: loss improved from 2.38287 to 2.31797, saving model to model.h5\n",
            "Epoch 8/100\n",
            "59825/59825 [==============================] - 38s 641us/step - loss: 2.2606\n",
            "\n",
            "Epoch 00008: loss improved from 2.31797 to 2.26062, saving model to model.h5\n",
            "Epoch 9/100\n",
            "59825/59825 [==============================] - 39s 649us/step - loss: 2.2133\n",
            "\n",
            "Epoch 00009: loss improved from 2.26062 to 2.21331, saving model to model.h5\n",
            "Epoch 10/100\n",
            "59825/59825 [==============================] - 38s 641us/step - loss: 2.1655\n",
            "\n",
            "Epoch 00010: loss improved from 2.21331 to 2.16552, saving model to model.h5\n",
            "Epoch 11/100\n",
            "59825/59825 [==============================] - 38s 643us/step - loss: 2.1265\n",
            "\n",
            "Epoch 00011: loss improved from 2.16552 to 2.12654, saving model to model.h5\n",
            "Epoch 12/100\n",
            "59825/59825 [==============================] - 38s 642us/step - loss: 2.0885\n",
            "\n",
            "Epoch 00012: loss improved from 2.12654 to 2.08853, saving model to model.h5\n",
            "Epoch 13/100\n",
            "59825/59825 [==============================] - 38s 642us/step - loss: 2.0532\n",
            "\n",
            "Epoch 00013: loss improved from 2.08853 to 2.05320, saving model to model.h5\n",
            "Epoch 14/100\n",
            "59825/59825 [==============================] - 38s 641us/step - loss: 2.0287\n",
            "\n",
            "Epoch 00014: loss improved from 2.05320 to 2.02870, saving model to model.h5\n",
            "Epoch 15/100\n",
            "59825/59825 [==============================] - 39s 646us/step - loss: 1.9990\n",
            "\n",
            "Epoch 00015: loss improved from 2.02870 to 1.99900, saving model to model.h5\n",
            "Epoch 16/100\n",
            "59825/59825 [==============================] - 38s 641us/step - loss: 1.9678\n",
            "\n",
            "Epoch 00016: loss improved from 1.99900 to 1.96783, saving model to model.h5\n",
            "Epoch 17/100\n",
            "59825/59825 [==============================] - 39s 649us/step - loss: 1.9481\n",
            "\n",
            "Epoch 00017: loss improved from 1.96783 to 1.94808, saving model to model.h5\n",
            "Epoch 18/100\n",
            "59825/59825 [==============================] - 39s 644us/step - loss: 1.9218\n",
            "\n",
            "Epoch 00018: loss improved from 1.94808 to 1.92182, saving model to model.h5\n",
            "Epoch 19/100\n",
            "59825/59825 [==============================] - 38s 643us/step - loss: 1.8973\n",
            "\n",
            "Epoch 00019: loss improved from 1.92182 to 1.89726, saving model to model.h5\n",
            "Epoch 20/100\n",
            "59825/59825 [==============================] - 39s 644us/step - loss: 1.8817\n",
            "\n",
            "Epoch 00020: loss improved from 1.89726 to 1.88172, saving model to model.h5\n",
            "Epoch 21/100\n",
            "59825/59825 [==============================] - 39s 648us/step - loss: 1.8563\n",
            "\n",
            "Epoch 00021: loss improved from 1.88172 to 1.85628, saving model to model.h5\n",
            "Epoch 22/100\n",
            "59825/59825 [==============================] - 38s 641us/step - loss: 1.8342\n",
            "\n",
            "Epoch 00022: loss improved from 1.85628 to 1.83419, saving model to model.h5\n",
            "Epoch 23/100\n",
            "59825/59825 [==============================] - 38s 643us/step - loss: 1.8220\n",
            "\n",
            "Epoch 00023: loss improved from 1.83419 to 1.82197, saving model to model.h5\n",
            "Epoch 24/100\n",
            "59825/59825 [==============================] - 38s 641us/step - loss: 1.7972\n",
            "\n",
            "Epoch 00024: loss improved from 1.82197 to 1.79716, saving model to model.h5\n",
            "Epoch 25/100\n",
            "59825/59825 [==============================] - 39s 646us/step - loss: 1.7772\n",
            "\n",
            "Epoch 00025: loss improved from 1.79716 to 1.77718, saving model to model.h5\n",
            "Epoch 26/100\n",
            "59825/59825 [==============================] - 39s 645us/step - loss: 1.7655\n",
            "\n",
            "Epoch 00026: loss improved from 1.77718 to 1.76550, saving model to model.h5\n",
            "Epoch 27/100\n",
            "59825/59825 [==============================] - 38s 642us/step - loss: 1.7518\n",
            "\n",
            "Epoch 00027: loss improved from 1.76550 to 1.75180, saving model to model.h5\n",
            "Epoch 28/100\n",
            "59825/59825 [==============================] - 38s 642us/step - loss: 1.7362\n",
            "\n",
            "Epoch 00028: loss improved from 1.75180 to 1.73617, saving model to model.h5\n",
            "Epoch 29/100\n",
            "59825/59825 [==============================] - 38s 640us/step - loss: 1.7206\n",
            "\n",
            "Epoch 00029: loss improved from 1.73617 to 1.72057, saving model to model.h5\n",
            "Epoch 30/100\n",
            "59825/59825 [==============================] - 38s 642us/step - loss: 1.7039\n",
            "\n",
            "Epoch 00030: loss improved from 1.72057 to 1.70387, saving model to model.h5\n",
            "Epoch 31/100\n",
            "59825/59825 [==============================] - 38s 639us/step - loss: 1.6979\n",
            "\n",
            "Epoch 00031: loss improved from 1.70387 to 1.69794, saving model to model.h5\n",
            "Epoch 32/100\n",
            "59825/59825 [==============================] - 38s 640us/step - loss: 1.6846\n",
            "\n",
            "Epoch 00032: loss improved from 1.69794 to 1.68456, saving model to model.h5\n",
            "Epoch 33/100\n",
            "59825/59825 [==============================] - 39s 644us/step - loss: 1.6600\n",
            "\n",
            "Epoch 00033: loss improved from 1.68456 to 1.65998, saving model to model.h5\n",
            "Epoch 34/100\n",
            "59825/59825 [==============================] - 39s 647us/step - loss: 1.6507\n",
            "\n",
            "Epoch 00034: loss improved from 1.65998 to 1.65072, saving model to model.h5\n",
            "Epoch 35/100\n",
            "59825/59825 [==============================] - 38s 642us/step - loss: 1.6409\n",
            "\n",
            "Epoch 00035: loss improved from 1.65072 to 1.64091, saving model to model.h5\n",
            "Epoch 36/100\n",
            "59825/59825 [==============================] - 39s 646us/step - loss: 1.6294\n",
            "\n",
            "Epoch 00036: loss improved from 1.64091 to 1.62944, saving model to model.h5\n",
            "Epoch 37/100\n",
            "59825/59825 [==============================] - 39s 644us/step - loss: 1.6083\n",
            "\n",
            "Epoch 00037: loss improved from 1.62944 to 1.60832, saving model to model.h5\n",
            "Epoch 38/100\n",
            "59825/59825 [==============================] - 39s 647us/step - loss: 1.6046\n",
            "\n",
            "Epoch 00038: loss improved from 1.60832 to 1.60460, saving model to model.h5\n",
            "Epoch 39/100\n",
            "59825/59825 [==============================] - 39s 644us/step - loss: 1.5984\n",
            "\n",
            "Epoch 00039: loss improved from 1.60460 to 1.59842, saving model to model.h5\n",
            "Epoch 40/100\n",
            "59825/59825 [==============================] - 39s 645us/step - loss: 1.5896\n",
            "\n",
            "Epoch 00040: loss improved from 1.59842 to 1.58963, saving model to model.h5\n",
            "Epoch 41/100\n",
            "59825/59825 [==============================] - 39s 644us/step - loss: 1.5782\n",
            "\n",
            "Epoch 00041: loss improved from 1.58963 to 1.57821, saving model to model.h5\n",
            "Epoch 42/100\n",
            "59825/59825 [==============================] - 39s 647us/step - loss: 1.5630\n",
            "\n",
            "Epoch 00042: loss improved from 1.57821 to 1.56299, saving model to model.h5\n",
            "Epoch 43/100\n",
            "59825/59825 [==============================] - 38s 641us/step - loss: 1.5539\n",
            "\n",
            "Epoch 00043: loss improved from 1.56299 to 1.55387, saving model to model.h5\n",
            "Epoch 44/100\n",
            "59825/59825 [==============================] - 38s 642us/step - loss: 1.5417\n",
            "\n",
            "Epoch 00044: loss improved from 1.55387 to 1.54166, saving model to model.h5\n",
            "Epoch 45/100\n",
            "59825/59825 [==============================] - 38s 641us/step - loss: 1.5329\n",
            "\n",
            "Epoch 00045: loss improved from 1.54166 to 1.53294, saving model to model.h5\n",
            "Epoch 46/100\n",
            "59825/59825 [==============================] - 38s 642us/step - loss: 1.5289\n",
            "\n",
            "Epoch 00046: loss improved from 1.53294 to 1.52886, saving model to model.h5\n",
            "Epoch 47/100\n",
            "59825/59825 [==============================] - 38s 640us/step - loss: 1.5167\n",
            "\n",
            "Epoch 00047: loss improved from 1.52886 to 1.51675, saving model to model.h5\n",
            "Epoch 48/100\n",
            "59825/59825 [==============================] - 38s 640us/step - loss: 1.5047\n",
            "\n",
            "Epoch 00048: loss improved from 1.51675 to 1.50471, saving model to model.h5\n",
            "Epoch 49/100\n",
            "59825/59825 [==============================] - 38s 641us/step - loss: 1.4960\n",
            "\n",
            "Epoch 00049: loss improved from 1.50471 to 1.49604, saving model to model.h5\n",
            "Epoch 50/100\n",
            "59825/59825 [==============================] - 39s 645us/step - loss: 1.4888\n",
            "\n",
            "Epoch 00050: loss improved from 1.49604 to 1.48881, saving model to model.h5\n",
            "Epoch 51/100\n",
            "59825/59825 [==============================] - 38s 640us/step - loss: 1.4787\n",
            "\n",
            "Epoch 00051: loss improved from 1.48881 to 1.47875, saving model to model.h5\n",
            "Epoch 52/100\n",
            "59825/59825 [==============================] - 38s 642us/step - loss: 1.4782\n",
            "\n",
            "Epoch 00052: loss improved from 1.47875 to 1.47818, saving model to model.h5\n",
            "Epoch 53/100\n",
            "59825/59825 [==============================] - 38s 641us/step - loss: 1.4655\n",
            "\n",
            "Epoch 00053: loss improved from 1.47818 to 1.46551, saving model to model.h5\n",
            "Epoch 54/100\n",
            "59825/59825 [==============================] - 38s 642us/step - loss: 1.4625\n",
            "\n",
            "Epoch 00054: loss improved from 1.46551 to 1.46249, saving model to model.h5\n",
            "Epoch 55/100\n",
            "59825/59825 [==============================] - 38s 641us/step - loss: 1.4508\n",
            "\n",
            "Epoch 00055: loss improved from 1.46249 to 1.45081, saving model to model.h5\n",
            "Epoch 56/100\n",
            "59825/59825 [==============================] - 39s 644us/step - loss: 1.4418\n",
            "\n",
            "Epoch 00056: loss improved from 1.45081 to 1.44180, saving model to model.h5\n",
            "Epoch 57/100\n",
            "59825/59825 [==============================] - 38s 641us/step - loss: 1.4376\n",
            "\n",
            "Epoch 00057: loss improved from 1.44180 to 1.43762, saving model to model.h5\n",
            "Epoch 58/100\n",
            "59825/59825 [==============================] - 39s 648us/step - loss: 1.4309\n",
            "\n",
            "Epoch 00058: loss improved from 1.43762 to 1.43087, saving model to model.h5\n",
            "Epoch 59/100\n",
            "59825/59825 [==============================] - 38s 643us/step - loss: 1.4274\n",
            "\n",
            "Epoch 00059: loss improved from 1.43087 to 1.42742, saving model to model.h5\n",
            "Epoch 60/100\n",
            "59825/59825 [==============================] - 39s 650us/step - loss: 1.4092\n",
            "\n",
            "Epoch 00060: loss improved from 1.42742 to 1.40924, saving model to model.h5\n",
            "Epoch 61/100\n",
            "59825/59825 [==============================] - 38s 642us/step - loss: 1.4069\n",
            "\n",
            "Epoch 00061: loss improved from 1.40924 to 1.40687, saving model to model.h5\n",
            "Epoch 62/100\n",
            "59825/59825 [==============================] - 42s 699us/step - loss: 1.3993\n",
            "\n",
            "Epoch 00062: loss improved from 1.40687 to 1.39929, saving model to model.h5\n",
            "Epoch 63/100\n",
            "59825/59825 [==============================] - 39s 654us/step - loss: 1.3979\n",
            "\n",
            "Epoch 00063: loss improved from 1.39929 to 1.39792, saving model to model.h5\n",
            "Epoch 64/100\n",
            "59825/59825 [==============================] - 38s 643us/step - loss: 1.3934\n",
            "\n",
            "Epoch 00064: loss improved from 1.39792 to 1.39338, saving model to model.h5\n",
            "Epoch 65/100\n",
            "59825/59825 [==============================] - 38s 636us/step - loss: 1.3849\n",
            "\n",
            "Epoch 00065: loss improved from 1.39338 to 1.38488, saving model to model.h5\n",
            "Epoch 66/100\n",
            "59825/59825 [==============================] - 39s 646us/step - loss: 1.3795\n",
            "\n",
            "Epoch 00066: loss improved from 1.38488 to 1.37947, saving model to model.h5\n",
            "Epoch 67/100\n",
            "59825/59825 [==============================] - 39s 653us/step - loss: 1.3702\n",
            "\n",
            "Epoch 00067: loss improved from 1.37947 to 1.37023, saving model to model.h5\n",
            "Epoch 68/100\n",
            "59825/59825 [==============================] - 38s 640us/step - loss: 1.3661\n",
            "\n",
            "Epoch 00068: loss improved from 1.37023 to 1.36608, saving model to model.h5\n",
            "Epoch 69/100\n",
            "59825/59825 [==============================] - 39s 646us/step - loss: 1.3503\n",
            "\n",
            "Epoch 00069: loss improved from 1.36608 to 1.35032, saving model to model.h5\n",
            "Epoch 70/100\n",
            "59825/59825 [==============================] - 38s 640us/step - loss: 1.3549\n",
            "\n",
            "Epoch 00070: loss did not improve from 1.35032\n",
            "Epoch 71/100\n",
            "59825/59825 [==============================] - 38s 640us/step - loss: 1.3391\n",
            "\n",
            "Epoch 00071: loss improved from 1.35032 to 1.33914, saving model to model.h5\n",
            "Epoch 72/100\n",
            "59825/59825 [==============================] - 38s 637us/step - loss: 1.3425\n",
            "\n",
            "Epoch 00072: loss did not improve from 1.33914\n",
            "Epoch 73/100\n",
            "59825/59825 [==============================] - 38s 639us/step - loss: 1.3320\n",
            "\n",
            "Epoch 00073: loss improved from 1.33914 to 1.33198, saving model to model.h5\n",
            "Epoch 74/100\n",
            "59825/59825 [==============================] - 39s 645us/step - loss: 1.3278\n",
            "\n",
            "Epoch 00074: loss improved from 1.33198 to 1.32781, saving model to model.h5\n",
            "Epoch 75/100\n",
            "59825/59825 [==============================] - 38s 639us/step - loss: 1.3242\n",
            "\n",
            "Epoch 00075: loss improved from 1.32781 to 1.32422, saving model to model.h5\n",
            "Epoch 76/100\n",
            "59825/59825 [==============================] - 38s 642us/step - loss: 1.3223\n",
            "\n",
            "Epoch 00076: loss improved from 1.32422 to 1.32225, saving model to model.h5\n",
            "Epoch 77/100\n",
            "59825/59825 [==============================] - 38s 642us/step - loss: 1.3163\n",
            "\n",
            "Epoch 00077: loss improved from 1.32225 to 1.31627, saving model to model.h5\n",
            "Epoch 78/100\n",
            "59825/59825 [==============================] - 39s 644us/step - loss: 1.3075\n",
            "\n",
            "Epoch 00078: loss improved from 1.31627 to 1.30746, saving model to model.h5\n",
            "Epoch 79/100\n",
            "59825/59825 [==============================] - 38s 639us/step - loss: 1.3028\n",
            "\n",
            "Epoch 00079: loss improved from 1.30746 to 1.30278, saving model to model.h5\n",
            "Epoch 80/100\n",
            "59825/59825 [==============================] - 38s 639us/step - loss: 1.2984\n",
            "\n",
            "Epoch 00080: loss improved from 1.30278 to 1.29844, saving model to model.h5\n",
            "Epoch 81/100\n",
            "59825/59825 [==============================] - 38s 637us/step - loss: 1.2969\n",
            "\n",
            "Epoch 00081: loss improved from 1.29844 to 1.29693, saving model to model.h5\n",
            "Epoch 82/100\n",
            "59825/59825 [==============================] - 38s 643us/step - loss: 1.2892\n",
            "\n",
            "Epoch 00082: loss improved from 1.29693 to 1.28917, saving model to model.h5\n",
            "Epoch 83/100\n",
            "59825/59825 [==============================] - 38s 637us/step - loss: 1.2802\n",
            "\n",
            "Epoch 00083: loss improved from 1.28917 to 1.28019, saving model to model.h5\n",
            "Epoch 84/100\n",
            "59825/59825 [==============================] - 38s 639us/step - loss: 1.2795\n",
            "\n",
            "Epoch 00084: loss improved from 1.28019 to 1.27952, saving model to model.h5\n",
            "Epoch 85/100\n",
            "59825/59825 [==============================] - 38s 638us/step - loss: 1.2713\n",
            "\n",
            "Epoch 00085: loss improved from 1.27952 to 1.27131, saving model to model.h5\n",
            "Epoch 86/100\n",
            "59825/59825 [==============================] - 38s 642us/step - loss: 1.2676\n",
            "\n",
            "Epoch 00086: loss improved from 1.27131 to 1.26758, saving model to model.h5\n",
            "Epoch 87/100\n",
            "59825/59825 [==============================] - 39s 646us/step - loss: 1.2644\n",
            "\n",
            "Epoch 00087: loss improved from 1.26758 to 1.26437, saving model to model.h5\n",
            "Epoch 88/100\n",
            "59825/59825 [==============================] - 38s 641us/step - loss: 1.2566\n",
            "\n",
            "Epoch 00088: loss improved from 1.26437 to 1.25659, saving model to model.h5\n",
            "Epoch 89/100\n",
            "59825/59825 [==============================] - 38s 637us/step - loss: 1.2524\n",
            "\n",
            "Epoch 00089: loss improved from 1.25659 to 1.25235, saving model to model.h5\n",
            "Epoch 90/100\n",
            "59825/59825 [==============================] - 39s 647us/step - loss: 1.2407\n",
            "\n",
            "Epoch 00090: loss improved from 1.25235 to 1.24068, saving model to model.h5\n",
            "Epoch 91/100\n",
            "59825/59825 [==============================] - 38s 639us/step - loss: 1.2469\n",
            "\n",
            "Epoch 00091: loss did not improve from 1.24068\n",
            "Epoch 92/100\n",
            "59825/59825 [==============================] - 38s 639us/step - loss: 1.2421\n",
            "\n",
            "Epoch 00092: loss did not improve from 1.24068\n",
            "Epoch 93/100\n",
            "59825/59825 [==============================] - 38s 637us/step - loss: 1.2468\n",
            "\n",
            "Epoch 00093: loss did not improve from 1.24068\n",
            "Epoch 94/100\n",
            "59825/59825 [==============================] - 38s 641us/step - loss: 1.2299\n",
            "\n",
            "Epoch 00094: loss improved from 1.24068 to 1.22992, saving model to model.h5\n",
            "Epoch 95/100\n",
            "59825/59825 [==============================] - 38s 637us/step - loss: 1.2238\n",
            "\n",
            "Epoch 00095: loss improved from 1.22992 to 1.22383, saving model to model.h5\n",
            "Epoch 96/100\n",
            "59825/59825 [==============================] - 38s 638us/step - loss: 1.2215\n",
            "\n",
            "Epoch 00096: loss improved from 1.22383 to 1.22147, saving model to model.h5\n",
            "Epoch 97/100\n",
            "59825/59825 [==============================] - 38s 641us/step - loss: 1.2251\n",
            "\n",
            "Epoch 00097: loss did not improve from 1.22147\n",
            "Epoch 98/100\n",
            "59825/59825 [==============================] - 38s 642us/step - loss: 1.2131\n",
            "\n",
            "Epoch 00098: loss improved from 1.22147 to 1.21306, saving model to model.h5\n",
            "Epoch 99/100\n",
            "59825/59825 [==============================] - 38s 640us/step - loss: 1.2101\n",
            "\n",
            "Epoch 00099: loss improved from 1.21306 to 1.21007, saving model to model.h5\n",
            "Epoch 100/100\n",
            "59825/59825 [==============================] - 38s 638us/step - loss: 1.2102\n",
            "\n",
            "Epoch 00100: loss did not improve from 1.21007\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2497da7518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25pUJQTPVXMg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = load_model('model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wpe-UmgVuIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuNPaJivGow-",
        "colab_type": "code",
        "outputId": "0b3a2f86-fb60-4d19-997f-4b92aae0b853",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "# pick a random seed\n",
        "import sys\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "start = 1231\n",
        "print(dataX[start])\n",
        "pattern = list(dataX[start])\n",
        "for value in pattern:\n",
        "  print(value, int_to_char[value])\n",
        "print(''.join([int_to_char[value] for value in pattern]))\n",
        "# print \"Seed:\"\n",
        "# print \"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\"\n",
        "# generate characters\n",
        "for i in range(500):\n",
        "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = numpy.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "# print \"\\nDone.\""
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[24 19  1 12  9 22 23  9 16 10  1  5 10 24  9 22  1 23 25  7 12  1  5  1\n",
            " 10  5 16 16  1  5]\n",
            "24 t\n",
            "19 o\n",
            "1  \n",
            "12 h\n",
            "9 e\n",
            "22 r\n",
            "23 s\n",
            "9 e\n",
            "16 l\n",
            "10 f\n",
            "1  \n",
            "5 a\n",
            "10 f\n",
            "24 t\n",
            "9 e\n",
            "22 r\n",
            "1  \n",
            "23 s\n",
            "25 u\n",
            "7 c\n",
            "12 h\n",
            "1  \n",
            "5 a\n",
            "1  \n",
            "10 f\n",
            "5 a\n",
            "16 l\n",
            "16 l\n",
            "1  \n",
            "5 a\n",
            "to herself after such a fall a\n",
            "nd sanes soees asd.whe qther iad gooedt whetes anl harder what sueste.bnice asd whe muryrds oeanled oe leave tueer hand interieted.tatulmlar and tuundd ay the sfppee uolether srlstamlsl anlce hopves wo tee sfe her hnyn mtoerie tucste tei swer tuermemes anl about forre taid alice as see no knce a doovowson of noteer deoied hirself.sutnswat all as m all sanpirg at vhe cooksttte ier iead triselinartty anice asd whe mooanes wo go cetteisly and tuierlnl whe sacbie and tatee ofar toeery anl toutai sut"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpBA19EiHZlT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}